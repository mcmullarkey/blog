[
  {
    "path": "posts/2021-05-18-im-opting-out/",
    "title": "I'm Opting Out",
    "description": "And I'm relieved",
    "author": [
      {
        "name": "Michael Mullarkey, PhD (Click to return to my website)",
        "url": "https://mcmullarkey.github.io"
      }
    ],
    "date": "2021-05-18",
    "categories": [],
    "contents": "\nI’ll Never Be a Professor, and I’m Relieved\nWhen I was 4 I wanted to be a cowboy,1 when I was 24 I wanted to be a professor, and now I don’t know what I want to be.  I’m opting out of the professor track. I’m not going to hustle 1st author papers, feverishly write grants as a PI, or apply to any professor jobs. Ever.\nBut You’ve Been So Privileged and Lucky at Each Step!\nYep, and I still want out. I have a privilege stack that goes halfway to the moon2 and I’ve already been “struck by lightning” level lucky at least three times.3  I also realize I’d have to be that lucky at least two more times4 if I wanted to stay on the professor track. I’ve gotten more realistic about the likelihood of that happening,5 but the main reason I’m opting out is a bit different.\nProfessor Is At Least 13 Jobs (I Counted)\nProfessors in clinical psychology are asked to:  1. Get grants6  2. Oversee/perform data collection7  3. Clean/preprocess data8  4. Store data in a safe, reliable way9  5. Analyze the data10  6. Write reports based on the data/publish them11  7. Publicize research results12  8. Review others’ research papers13  9. Review others’ grants14  That’s already so much! But wait, that’s only the research part of the process. You also have to:  10. Interview/hire graduate students and staff15  11. Mentor students/staff16  12. Provide clinical supervision/consultation/see patients17  13. Teach classes18  I also know for sure I’m leaving out other important roles clinical psychology professors play. And I was still able to come up with at least 13 separate jobs without thinking very hard. Check the footnotes next to each “role” for the entire job that “role” represents in other organizations.19  I was more willing to try to run this gauntlet when I believed being a professor was the best way for me to do rigorous research that improves mental health. I don’t believe that anymore.\nI’ve Lost Faith Being a Professor is a Reliable, Repeatable Path to Improving Mental Health\nMental health outcomes haven’t gotten better in decades. We’ve told ourselves complicated stories about why our “cutting-edge” research isn’t translating to better outcomes for people.20 But there’s a much simpler story that’s much harder for us to hear.  Our studies almost never have enough people in them to know how good our treatments actually are,21 our research overwhelmingly exclude folks with minoritized identities,22 and our efforts to disseminate these shakier-than-we-want-to-admit treatments often end when the grant does.  Doing clinical intervention research well is devastatingly hard23 even if all the incentives are aligned toward helping people. Our current scientific culture + incentives set up people to never help a single patient outside of research. You are fighting uphill to do just one of increasing sample size, conducting research among folks with any minoritized identities,24 or setting up infrastructure that makes treatment dissemination sustainable. All three at once? Yikes.  And even if you can run that study, you still have to clean the data, analyze it, and report what actually happened. If we want to make people’s lives better we have to report when all of this doesn’t work too.25 However, reporting “we tried, but it didn’t work” can make getting grants/publications even harder.26  For me, improving mental health through doing research as a lone PI with all these “roles” looks like fitting a camel through the eye of a needle. I care deeply about wider ranging, more equitable access to rigorously tested mental health treatments. I’m opting out because I care about more equitable access to effective treatment. I think opting out will make it more likely I can be useful toward making that reality happen, not less.27\nThis is My Choice, Not a Condemnation\nThere are people in this space who somehow manage to do all of these jobs while working toward increasing equity, rigor, and beneficial outcomes for patients. I’ve been lucky enough to work for two of them. I also don’t think it’s a coincidence that both of them were flush with grant money. They could afford to reduce the number of roles they had to play, and even then they are still doing so much.28  I’m not saying no one should be a professor. I’m not pretending I have all29 the answers, and I totally understand if people disagree with my assessment of the field.30 Or they agree with my assessment and decide they can best improve patient outcomes by staying on the PI track.31  I would be thrilled to be wrong about the state of the field in as many ways as possible. Even if I’m right, I hope there are PIs inside this shot to hell system who keep fighting to make academic clinical psychology a more equitable, rigorous, and beneficial-to-patients space.  And I also realize those folks are adding at least one more full-time job to their pile.\nWhat’s Next?\nI know what I want to do. Despite academia’s32 best efforts, I’ve learned to separate what I want to do from a specific job title. I’ll still be trying to solve hard data problems, working to increase mental health treatment access/effectiveness, and talking shit on Twitter about how our science isn’t good enough33 to actually help people yet.  In the short-term, I’ve transitioned into the Director of Data Science role at the Lab for Scalable Mental Health with Dr. Jessica Schleider. I’m super excited to be involved with clinical trials that are large enough to learn what we want to know, accessible to folks with all kinds of intersecting identities, and include treatments that are already being actively disseminated in multiple ways. This way I can primarily focus on doing jobs 2-5 in the list above34 rather than all 13.\nWe Don’t Have to Do 13 Jobs to Be Good Enough\nI’ve made peace with the fact that while I kind of like all 13 of those jobs, there are some I vastly prefer to others and that doesn’t mean I’m not good enough. I’d much rather spend my time trying to figure out a difficult data science problem than write another cover letter for a paper that’s already been rejected four times.35  I’m taking both weekend days off for the first time in an embarrassingly long time. I have time to learn new skills36 that used to consistently fall by the wayside. I’m competing in a live-streamed data science competition.37 Come August, I’ll get a substantial raise that’s still nowhere close to what I might be able to get if I had jumped all the way out of academia. All of those are quality of life improvements that made my decision a lot easier.\nWe Don’t Have to Stay\nI’ve also made peace with the near certainty of leaving academia entirely at some point. Even at a salary point much lower than industry’s, my PI can only guarantee my salary for 1 year. This isn’t my PI’s fault, and she’s fought for me every step of the way. But in order to fund a data scientist position you have to win the grant lottery consistently, and you also have to convince the funder to actually pay the data scientist more than a postdoc. There’s no guarantee any of the grants we apply for will happen at all. Even if one of the grants works out there’s no guarantee they’ll agree to pay me a reasonable salary. Rinse that and repeat every funding cycle, and the math gets really hard. So, there’s a decent chance mental health treatment access/effectiveness could become outside projects rather than 9-5s for me. I’m ok with that. In fact, I get really excited when I think about doing a job where I’m solving problems with 100s of thousands of data points rather than a few thousand at best. If you’d like to talk about consulting roles or data scientist roles, feel free to check out my consulting page here and my resume here. You can get in touch with me via e-mail here.  I’ve made the calculation that I’m more likely to improve people’s mental health by attempting some of my outside projects and “failing” than becoming a PI and “succeeding.” I might be hilariously wrong about that, but I’m excited to see how those projects go.38\nWhat Does This All Mean?\nWhatever you want it to, and I really just want what’s best for you. If you’re on the fence about whether you have to stay in academia or not, I genuinely don’t know what you should do. I think there are serious problems with academia, but it’s not like industry, private practice, or anywhere else is perfect. What I’m proudest of: making decisions prioritizing what I wanted to do over whether I had the title/trajectory I’d been conditioned to want.  I feel simultaneously like I’ve said way too much and not nearly enough. There’s a chance I’ll talk more about this decision at some point, but I’m going to leave it here for now. I hope you’re doing as well as you can be as you read this, and if you’re not you deserve better circumstances.\n\nor Robin Hood↩︎\nstraight white dude, grew up well off, have great/supportive parents (one of whom has a PhD while the other was the first woman student body president in her university’s history), behaviorally supportive advisers at the PhD/postdoc level, the list goes on↩︎\ngetting into a PhD program in clinical psychology after receiving 0 official interviews during my application cycle, matching to the one internship site where I could live in the same physical place as my partner, and getting a dream postdoc where I could also live with my partner↩︎\nBoth my partner and I getting tenure-track jobs in the same general location and then both of us getting tenure↩︎\npreposterously unlikely! seriously y’all↩︎\nJob = Grant Writer↩︎\nJob = Head of Survey Firm↩︎\nJob = Data Engineer↩︎\nJob = Data Manager↩︎\nJob = Data Scientist/Statistician↩︎\nJob = Research Scientist/Technical Writer↩︎\nJob = Science Communicator/Public Relations↩︎\nJob = Copy editor (just kidding, Editor)↩︎\nJob = Grant Reviewer (yes, that’s a job)↩︎\nJob = Recruiter/Hiring Manager↩︎\nJob = Manager↩︎\nJob = Licensed Mental Health Provider↩︎\nJob = Teacher↩︎\nI understand too much atomization of people’s jobs also isn’t great, and I think it’s worth thinking about how many of these “roles” we ask professors to play↩︎\nThese often involve publishing even more papers with “Research-Practice” gap somewhere in the title↩︎\nmuch less which treatments work best for which people↩︎\nfrom the researchers to the participants↩︎\ndid you see that list of 13 jobs??↩︎\nmuch less intersecting ones↩︎\nwhich is likely the norm rather than the exception↩︎\nI believe it’s easier to publish these kinds of results than it was 10 years ago, especially if you have resources, but it’s still a far cry from easy↩︎\na bit more on this later in the post↩︎\nyes, both of them are amazing humans with incredible skills… and they could also hire lots of staff + buy themselves out of some other obligations↩︎\nor any of↩︎\nand they could disagree about whether the field incentivizes helping people, whether the field should be concerned with directly helping people, etc.↩︎\nmy values and skills are not the same as others’ values/skills, and it’s going to take at least several villages working together to get us where we need to go↩︎\nand capitalism’s↩︎\nit just isn’t y’all, and it’s not even close↩︎\nwith some dashes of others↩︎\nyep, not a fully fair comparison, but it actually kind of is↩︎\nBayesian stats, Python, and others!↩︎\nthat will almost certainly be a humbling, yet awesome experinece!↩︎\nnone of these are ready for primetime yet, and as soon as they are I’m excited to share them with everyone!↩︎\n",
    "preview": {},
    "last_modified": "2021-05-18T17:27:38-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-17-data-distillery-sip-a-part-ii-of-data-distillery-dash-episode-02/",
    "title": "Data Distillery Sip: Part II of Data Distillery Dash Episode 02",
    "description": "Why are we fitting two predictive models instead of just one?",
    "author": [
      {
        "name": "Michael Mullarkey, PhD (Click to return to my website)",
        "url": "https://mcmullarkey.github.io"
      }
    ],
    "date": "2021-05-17",
    "categories": [],
    "contents": "\nWhat is Data Distillery Sip?\nSometimes, I do a livestream/blog post combo called Data Distillery Dash. I take a research topic at random1 or dataset I’ve never seen2, wrangle the data, create a predictive model or two, and write up a blog post: all within 2 hours live on Twitch! All the words and code in the part I post were written during an ~2 hour livestream. If you want to come join the party, I plan on doing this on Thursdays from 3:30-5:30 EST here.  Sometimes, I’ll want to do extensions on what I accomplished during the Data Distillery Dash, but at a (slightly) more leisurely pace. I’ve decided to call those posts Data Distillery Sips. This is the first one, and also serves as a continuing tutorial on how to build a predictive pipeline for psychological interventions.\nWhere Did We Leave Off/Why Were We Doing That?\nAt the end of the last post, we had built predictive models within the intervention and control groups of an online intervention. We had to build these models separately because knowing which intervention would be better for someone requires estimating two things:  1. How well we’d predict the person would respond to the intervention they actually received  2. How well we’d predict the person would respond to the intervention they didn’t actually receive  If that feels confusing, it’s ok! To understand why we need to do this, imagine we have access to a time machine3. First, we give someone one of these online interventions, then measure how well they respond to that intervention. Then, we use our time machine to go back and give the same person the other intervention and measure how well they respond to that intervention. We then compare how much they improved in one timeline/intervention vs. another, and we could know which intervention helped them more.  But we don’t have access to that time machine4 so we can’t know for sure which intervention would benefit somebody more. However, since folks in this study were randomized to different groups, that means people from both groups are statistically exchangeable. Dr. Darren Dahly has an excellent post on this concept, but the key takeaway is: randomization means we can assume the distribution of future outcomes is the same in both groups. In other words, we can assume that absent any intervention the distribution of how well people would do over time is equivalent across groups. Approximately equivalent numbers of folks would get better, stay the same, or worsen on the outcome we care about. \nWhat Does Exchangeability Buy Us?\nThis exchangeability allows us to use our models to make predictions as if both groups had received both interventions. Then, we can see which model predicts more improvement for any given person. Some participants will be “lucky” and have been randomized to the intervention our models predict would be more optimal for them. Some participants will be “unlucky” and have been randomized to the intervention our models predict would be less effective for them. In this very specific way, our models are like very buggy, fickle time machines.  The tempting5 way to conduct this next step would be to reuse the same data we used to make our predictions. We could test if the “lucky” participants improved more than the “unlucky” participants within the same data. However, we’d be testing our predictions on the same data used to generate those predictions. This is a huge no-no in prediction-focused spaces. Showing our predictions generalize only to the data that produced the predictions isn’t that impressive. We want to see if those predictions can generalize to data the model has never seen before6.\nThe Importance of the Hold Out Set\nLuckily, we created a “test” or “hold out”7 set of 80 people that weren’t used to train the initial model. We’ll want to apply the predictions of both models to those 80 people, determine who was “lucky” vs. “unlucky” and then compare whether the “lucky” group actually improved more than the “unlucky” group.\nWhat We’ll Get To in Part 3\nThis sip has been mroe about why we’re doing what we’re doing. The next post will contain the code that makes this happen!\n\nYikes↩︎\ndouble yikes↩︎\nI promise this is relevant, stay with me!↩︎\nor if you do you should probably tell somebody↩︎\nand common!↩︎\nthere might, huge emphasis on might, be a way to reuse the same data as an initial check on the model using only the ‘out of fold’ predictions, but I’m still looking into how feasible/responsible that would be↩︎\nthere are lots of names that mean the same thing in machine learning, statistics, etc.↩︎\n",
    "preview": {},
    "last_modified": "2021-05-17T12:51:11-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-13-data-distillery-dash-episode-02/",
    "title": "Data Distillery Dash Episode 02",
    "description": "The start of a prediction pipeline for psychological interventions.",
    "author": [
      {
        "name": "Michael Mullarkey, PhD (Click to return to my website)",
        "url": "https://mcmullarkey.github.io"
      }
    ],
    "date": "2021-05-13",
    "categories": [],
    "contents": "\nWhat Is Data Distillery Dash?\nI take a research topic at random1 or dataset I’ve never seen2, wrangle the data, create a predictive model or two, and write up a blog post: all within 2 hours live on Twitch! All the words and code in this post were written during an ~2 hour livestream. If you want to come join the party, I plan on doing this on Thursdays from 3:30-5:30 EST here.\nWhat Data Are We Using Today?\nWe’re going to predict subjective feelings of being ready to go back to work at the end of an online randomized clinical trial!3 I learned about this data from a paper by Dr. Nick Jacobson and the data is from this paper. This is mostly so I can practice setting up a predictive modeling pipeline that allows us to predict who would benefit more from one condition or another in a clinical trial. Even in situations like this where the control group seems minimal, they can actually have some therapeutic benefit. In fact, sometimes people seem to respond better to what we’ve described as psychological placebos compared to what we’ve described as active treatments!4  Let’s start with what we need to do for any predictive modeling pipleline: data cleaning/munging! This is all the more important here because the data was stored in a .sav SPSS data format. This means we need to read_sav from the haven package to read the data in, and to get a sense of what the variables actually are we use the view_df function from the sjPlot package.  I also decided to not use any of the non-collected at baseline variables in this particular modeling pipeline. Why? Right now I’m most interested in better matching people to treatments before they start. I would want to use the data over time if I was more interested in trying to tailor the treatment as it was being delivered. Even though it might not look like much, cleaning/munging the data (especially since I’d never seen it before!) took a lot of the time during this stream.\n\n\nlibrary(haven)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(janitor)\n\ndata <- read_sav(\"online_rct.sav\") %>% \n  clean_names()\n\n# glimpse(data)\n\nlibrary(sjPlot)\n\nview_df(data)\n\n\n\nData frame: data\n\n\nID\n\n\nName\n\n\nLabel\n\n\nValues\n\n\nValue Labels\n\n\n1\n\n\nimputation\n\n\nImputation code\n\n\n0\n\n\noriginal data\n\n\n2\n\n\ngruppenart\n\n\nStudy group\n\n\n12\n\n\nCGIG\n\n\n3\n\n\nsibar1bt0\n\n\nAge\n\n\nrange: 25-59\n\n\n4\n\n\nsibar1at0\n\n\nSex\n\n\n12\n\n\nfemalemale\n\n\n5\n\n\nindikation\n\n\nMedical Indication\n\n\n123\n\n\npsychosomaticcardiologyorthopedic\n\n\n6\n\n\nschulab\n\n\nEducation\n\n\n123\n\n\n9 years10 years13 years\n\n\n7\n\n\nbehandlungsdauer\n\n\nTime between intake and discharge (weeks)\n\n\nrange: 2-8\n\n\n8\n\n\nsibar5at1\n\n\nDuration of sick leaves in the last 12 months(weeks)\n\n\nrange: 0.0-58.0\n\n\n9\n\n\narbeitsunf\n\n\nStatus of work ability at T1 (physicians’ rating)\n\n\n013459\n\n\nMaßnahme nicht ordnungsgemäß abgeschlossen, gestorbenarbeitsfähigarbeitsunfähigKinder-RehaHausfrau / HausmannBeurteilung nicht erforderlich (Altersrentner, Angehöriger)\n\n\n10\n\n\nleistft1\n\n\nWork capacity T1\n\n\n12\n\n\nfully capablelimited to not at all capable\n\n\n11\n\n\nrentet1\n\n\nApplication for pension T1\n\n\n12\n\n\napplied or plans to applynot applied and no plans\n\n\n12\n\n\nsibar2t1\n\n\nDo you consider your work ability permanentlythreatened by your health status?\n\n\n12\n\n\nyesno\n\n\n13\n\n\nsibar5t1\n\n\nWhen you consider your current health and workability: Do you believe that you will be workinguntil retirement age?\n\n\n12\n\n\nyesno\n\n\n14\n\n\nlogin\n\n\nLogin after discharge\n\n\n012\n\n\nnoyesnot yet\n\n\n15\n\n\ntn_blog\n\n\nParticipation intervention (min. 1 Blog)\n\n\n01\n\n\nnoyes\n\n\n16\n\n\nblogs\n\n\nNumber of blogs\n\n\nrange: 0-13\n\n\n17\n\n\nzufig1at2\n\n\nFrequency Reading blogs (IG)\n\n\n1234\n\n\ndailyseveral times a weekseldomnever\n\n\n18\n\n\nzufig1bt2\n\n\nFrequency Reading therapeutic feedback (IG)\n\n\n1234\n\n\ndailyseveral times a weekseldomnever\n\n\n19\n\n\nzufig1ct2\n\n\nFrequency Using work sheets (IG)\n\n\n1234\n\n\ndailyseveral times a weekseldomnever\n\n\n20\n\n\nzufig1dt2\n\n\nFrequency Listening to relaxation techniques (IG)\n\n\n1234\n\n\ndailyseveral times a weekseldomnever\n\n\n21\n\n\nzufig1et2\n\n\nFrequency Reading posts in the forum (IG)\n\n\n1234\n\n\ndailyseveral times a weekseldomnever\n\n\n22\n\n\nzufig1ft2\n\n\nFrequency Writing post in the forum (IG)\n\n\n1234\n\n\ndailyseveral times a weekseldomnever\n\n\n23\n\n\nzufig2at2\n\n\nHelpful Writing blogs (IG)\n\n\n15\n\n\nnot at allvery\n\n\n24\n\n\nzufig2bt2\n\n\nHelpful Reading therapeutic feedback (IG)\n\n\n15\n\n\nnot at allvery\n\n\n25\n\n\nzufig2ct2\n\n\nHelpful Using work sheets (IG)\n\n\n15\n\n\nnot at allvery\n\n\n26\n\n\nzufig2dt2\n\n\nHelpful Listening to relaxation techniques (IG)\n\n\n15\n\n\nnot at allvery\n\n\n27\n\n\nzufig2et2\n\n\nHelpful Reading posts in the forum (IG)\n\n\n15\n\n\nnot at allvery\n\n\n28\n\n\nzufig2ft2\n\n\nHelpful Writing posts in the forum (IG)\n\n\n15\n\n\nnot at allvery\n\n\n29\n\n\nzufig2gt2\n\n\nHelpful Online aftercare alltogether (IG)\n\n\n15\n\n\nnot at allvery\n\n\n30\n\n\nzufig3t2\n\n\nHelpful Alliance with online therapist (IG)\n\n\n15\n\n\nnot at allvery\n\n\n31\n\n\nzufkg1at2\n\n\nFrequency Stress management info (CG)\n\n\n1234\n\n\ndailyseveral times a weekseldomnever\n\n\n32\n\n\nzufkg1bt2\n\n\nFrequency Healthy diet info (CG)\n\n\n1234\n\n\ndailyseveral times a weekseldomnever\n\n\n33\n\n\nzufkg1ct2\n\n\nFrequency Sports/ physical activity info (CG)\n\n\n1234\n\n\ndailyseveral times a weekseldomnever\n\n\n34\n\n\nzufkg1dt2\n\n\nFrequency Sleep info (CG)\n\n\n1234\n\n\ndailyseveral times a weekseldomnever\n\n\n35\n\n\nzufkg1et2\n\n\nFrequency Relaxation info (CG)\n\n\n1234\n\n\ndailyseveral times a weekseldomnever\n\n\n36\n\n\nzufkg2at2\n\n\nHelpful Stress management info (CG)\n\n\n15\n\n\nnot at allvery\n\n\n37\n\n\nzufkg2bt2\n\n\nHelpful Healthy diet info (CG)\n\n\n15\n\n\nnot at allvery\n\n\n38\n\n\nzufkg2ct2\n\n\nHelpful Sports/ physical activity info (CG)\n\n\n15\n\n\nnot at allvery\n\n\n39\n\n\nzufkg2dt2\n\n\nHelpful Sleep info (CG)\n\n\n15\n\n\nnot at allvery\n\n\n40\n\n\nzufkg2et2\n\n\nHelpful Relaxation info (CG)\n\n\n15\n\n\nnot at allvery\n\n\n41\n\n\nzufkg2ft2\n\n\nHelpful Online aftercare alltogether (CG)\n\n\n15\n\n\nnot at allvery\n\n\n42\n\n\nzufges_t2_dichotom\n\n\nDichotomised Helpfulness Online aftercarealltogether\n\n\nrange: 1-2\n\n\n43\n\n\nspe_sum_t0\n\n\nSumscore SPE T0\n\n\nrange: 0-3\n\n\n44\n\n\nspe_sum_t1\n\n\nSumscore SPE T1\n\n\nrange: 0-3\n\n\n45\n\n\nspe_sum_t2\n\n\nSumscore SPE T2\n\n\nrange: 0-3\n\n\n46\n\n\nspe_sum_t3\n\n\nSumscore SPE T3\n\n\nrange: 0-3\n\n\n47\n\n\nphq_dept0\n\n\nSumscore (0-27) depressive symptoms (PHQ-9) T0\n\n\nrange: 0.0-27.0\n\n\n48\n\n\nphq_dept1\n\n\nSumscore (0-27) depressive symptoms (PHQ-9) T1\n\n\nrange: 0.0-27.0\n\n\n49\n\n\nphq_dept2\n\n\nSumscore (0-27) depressive symptoms (PHQ-9) T2\n\n\nrange: 0.0-27.0\n\n\n50\n\n\nphq_dept3\n\n\nSumscore (0-27) depressive symptoms (PHQ-9) T3\n\n\nrange: 0.0-27.0\n\n\n51\n\n\nphq_som_t0\n\n\nSumscore (0-30) somatoform symptoms (PHQ-15) T0\n\n\nrange: 0.0-26.8\n\n\n52\n\n\nphq_som_t1\n\n\nSumscore (0-30) somatoform symptoms (PHQ-15) T1\n\n\nrange: 0.0-26.0\n\n\n53\n\n\nphq_som_t2\n\n\nSumscore (0-30) somatoform symptoms (PHQ-15) T2\n\n\nrange: 0.0-27.7\n\n\n54\n\n\nphq_som_t3\n\n\nSumscore (0-30) somatoform symptoms (PHQ-15) T3\n\n\nrange: 0.0-26.5\n\n\n55\n\n\nphq_stress_t0\n\n\nSumscore (0-20) PHQ-stress T0\n\n\nrange: 0.0-19.0\n\n\n56\n\n\nphq_stress_t1\n\n\nSumscore (0-20) PHQ-stress T1\n\n\nrange: 0.0-20.0\n\n\n57\n\n\nphq_stress_t2\n\n\nSumscore (0-20) PHQ-stress T2\n\n\nrange: 0.0-20.0\n\n\n58\n\n\nphq_stress_t3\n\n\nSumscore (0-20) PHQ-stress T3\n\n\nrange: 0.0-20.0\n\n\n59\n\n\ngad7t0\n\n\nSumscore (0-21) generalized anxiety (GAD-7) T0\n\n\nrange: 0.0-21.0\n\n\n60\n\n\ngad7t1\n\n\nSumscore (0-21) generalized anxiety (GAD-7) T1\n\n\nrange: 0-21\n\n\n61\n\n\ngad7t2\n\n\nSumscore (0-21) generalized anxiety (GAD-7) T2\n\n\nrange: 0-21\n\n\n62\n\n\ngad7t3\n\n\nSumscore (0-21) generalized anxiety (GAD-7) T3\n\n\nrange: 0-21\n\n\n63\n\n\nksk12t0\n\n\nSF-12 Physical Component Summary (T0)\n\n\nrange: 12.0-67.0\n\n\n64\n\n\npsk12t0\n\n\nSF-12 Mental Component Summary (T0)\n\n\nrange: 7.0-70.0\n\n\n65\n\n\nksk12t1\n\n\nSF-12 Physical Component Summary (T1)\n\n\nrange: 17.0-72.0\n\n\n66\n\n\npsk12t1\n\n\nSF-12 Mental Component Summary (T1)\n\n\nrange: 10.0-78.0\n\n\n67\n\n\nksk12t2\n\n\nSF-12 Physical Component Summary (T2)\n\n\nrange: 5.0-80.0\n\n\n68\n\n\npsk12t2\n\n\nSF-12 Mental Component Summary (T2)\n\n\nrange: 5.0-72.0\n\n\n69\n\n\nksk12t3\n\n\nSF-12 Physical Component Summaryt (T3)\n\n\nrange: 10.0-74.0\n\n\n70\n\n\npsk12t3\n\n\nSF-12 Mental Component Summary (T3)\n\n\nrange: 9.0-79.0\n\n\n71\n\n\nav_n1_t0\n\n\nAVEM Subjective importance of work T0(Stanine-Werte)\n\n\nrange: 1-9\n\n\n72\n\n\nav_n2_t0\n\n\nAVEM Work related ambition T0 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n73\n\n\nav_n3_t0\n\n\nAVEM Willingness to work until exhausted T0(Stanine-Werte)\n\n\nrange: 1-9\n\n\n74\n\n\nav_n4_t0\n\n\nAVEM Striving for perfection T0 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n75\n\n\nav_n5_t0\n\n\nAVEM Distancing ability T0 (Stanine-Werte)\n\n\nrange: 2-8\n\n\n76\n\n\nav_n6_t0\n\n\nAVEM Tendency to resignation T0 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n77\n\n\nav_n7_t0\n\n\nAVEM Proactive problem solving T0 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n78\n\n\nav_n8_t0\n\n\nAVEM Inner calm and balance T0 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n79\n\n\nav_n9_t0\n\n\nAVEM Experience of success at work T0(Stanine-Werte)\n\n\nrange: 1-9\n\n\n80\n\n\nav_n10_t0\n\n\nAVEM Satisfaction with life T0 (Stanine-Werte)\n\n\nrange: 1-8\n\n\n81\n\n\nav_n11_t0\n\n\nAVEM Experience of social support T0(Stanine-Werte)\n\n\nrange: 1-8\n\n\n82\n\n\nav_n1_t1\n\n\nAVEM Subjective importance of work T1(Stanine-Werte)\n\n\nrange: 1-9\n\n\n83\n\n\nav_n2_t1\n\n\nAVEM Work related ambition T1 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n84\n\n\nav_n3_t1\n\n\nAVEM Willingness to work until exhausted T1(Stanine-Werte)\n\n\nrange: 1-9\n\n\n85\n\n\nav_n4_t1\n\n\nAVEM Striving for perfection T1 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n86\n\n\nav_n5_t1\n\n\nAVEM Distancing ability T1 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n87\n\n\nav_n6_t1\n\n\nAVEM Tendency to resignation T1 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n88\n\n\nav_n7_t1\n\n\nAVEM Proactive problem solving T1 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n89\n\n\nav_n8_t1\n\n\nAVEM Inner calm and balance T1 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n90\n\n\nav_n9_t1\n\n\nAVEM Experience of success at work T1(Stanine-Werte)\n\n\nrange: 1-9\n\n\n91\n\n\nav_n10_t1\n\n\nAVEM Satisfaction with life T1 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n92\n\n\nav_n11_t1\n\n\nAVEM Experience of social support T1(Stanine-Werte)\n\n\nrange: 1-8\n\n\n93\n\n\nav_n1_t2\n\n\nAVEM Subjective importance of work T2(Stanine-Werte)\n\n\nrange: 1-9\n\n\n94\n\n\nav_n2_t2\n\n\nAVEM Work related ambition T2 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n95\n\n\nav_n3_t2\n\n\nAVEM Willingness to work until exhausted T2(Stanine-Werte)\n\n\nrange: 1-9\n\n\n96\n\n\nav_n4_t2\n\n\nAVEM Striving for perfection T2 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n97\n\n\nav_n5_t2\n\n\nAVEM Distancing ability T2 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n98\n\n\nav_n6_t2\n\n\nAVEM Tendency to resignation T2 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n99\n\n\nav_n7_t2\n\n\nAVEM Proactive problem solving T2 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n100\n\n\nav_n8_t2\n\n\nAVEM Inner calm and balance T2 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n101\n\n\nav_n9_t2\n\n\nAVEM Experience of success at work T2(Stanine-Werte)\n\n\nrange: 1-9\n\n\n102\n\n\nav_n10_t2\n\n\nAVEM Satisfaction with life T2 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n103\n\n\nav_n11_t2\n\n\nAVEM Experience of social support T2(Stanine-Werte)\n\n\nrange: 1-9\n\n\n104\n\n\nav_n1_t3\n\n\nAVEM Subjective importance of work T3(Stanine-Werte)\n\n\nrange: 1-9\n\n\n105\n\n\nav_n2_t3\n\n\nAVEM Work related ambition T3 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n106\n\n\nav_n3_t3\n\n\nAVEM Willingness to work until exhausted T3(Stanine-Werte)\n\n\nrange: 1-9\n\n\n107\n\n\nav_n4_t3\n\n\nAVEM Striving for perfection T3 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n108\n\n\nav_n5_t3\n\n\nAVEM Distancing ability T3 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n109\n\n\nav_n6_t3\n\n\nAVEM Tendency to resignation T3 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n110\n\n\nav_n7_t3\n\n\nAVEM Proactive problem solving T3 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n111\n\n\nav_n8_t3\n\n\nAVEM Inner calm and balance T3 (Stanine-Werte)\n\n\nrange: 1-9\n\n\n112\n\n\nav_n9_t3\n\n\nAVEM Experience of success at work T3(Stanine-Werte)\n\n\nrange: 1-9\n\n\n113\n\n\nav_n10_t3\n\n\nAVEM Satisfaction with life T3 (Stanine-Werte)\n\n\nrange: 1-8\n\n\n114\n\n\nav_n11_t3\n\n\nAVEM Experience of social support T3(Stanine-Werte)\n\n\nrange: 1-9\n\n\n115\n\n\nsibar_risk\n\n\nSIBAR Risk Score (0-19, Cutoff 8) T0\n\n\nrange: 0.0-16.0\n\n\n116\n\n\nsibar_risk_di\n\n\nSIBAR Risc Score > 8 T0\n\n\n01\n\n\nnoyes\n\n\n117\n\n\nsibar_risk_t1\n\n\nSIBAR Risk Score (0-19, Cutoff 8) T1\n\n\nrange: 0.0-16.0\n\n\n118\n\n\nsibar_risk_di_t1\n\n\nSIBAR Risc Score > 8 T1\n\n\n01\n\n\nnoyes\n\n\n119\n\n\nsibar_riskt2\n\n\nSIBAR Risk Score (0-19, Cutoff 8) T2\n\n\nrange: 0.0-16.0\n\n\n120\n\n\nsibar_riskdi_t2\n\n\nSIBAR Risc Score > 8 T2\n\n\n01\n\n\nnoyes\n\n\n121\n\n\nsibar_riskt3\n\n\nSIBAR Risk Score (0-19, Cutoff 8) T3\n\n\nrange: 0.0-17.0\n\n\n122\n\n\nsibar_riskdi_t3\n\n\nSIBAR Risc Score > 8 T3\n\n\n01\n\n\nnoyes\n\n\ndata_init <- data %>%\n  rename(cond = gruppenart) %>% \n  dplyr::select(-arbeitsunf,\n                -ends_with(\"_t1\"), -ends_with(\"_t2\"),\n                -ends_with(\"t1\"), -ends_with(\"t2\"),\n                -ends_with(\"1\"),-ends_with(\"2\"),\n                -ends_with(\"_t3\"),\n                -ends_with(\"t3\"),\n                -ends_with(\"3\"),\n                -login, -tn_blog,\n                -blogs, -zufges_t2_dichotom) %>% \n  glimpse()\n\n\nRows: 6,952\nColumns: 27\n$ imputation       <dbl+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ cond             <dbl+lbl> 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, …\n$ sibar1bt0        <dbl> 48, 47, 48, 47, 40, 31, 53, 43, 45, 54, 48,…\n$ sibar1at0        <dbl+lbl> 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, …\n$ indikation       <dbl+lbl> 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 2, 1, …\n$ schulab          <dbl+lbl> 1, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 2, …\n$ behandlungsdauer <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ spe_sum_t0       <dbl> 3, 3, 3, 2, 3, 0, 1, 1, 0, 0, 2, 1, 2, 3, 0…\n$ phq_dept0        <dbl> 20.00, 21.00, 20.25, 25.00, 23.00, 21.00, 1…\n$ phq_som_t0       <dbl> 23.571429, 18.000000, 22.500000, 26.785714,…\n$ phq_stress_t0    <dbl> 11.00000, 16.00000, 15.55556, 17.00000, 15.…\n$ gad7t0           <dbl> 17, 15, 19, 21, 15, 13, 10, 13, 9, 12, 5, 6…\n$ ksk12t0          <dbl> 32.39855, 31.59306, 24.46546, 50.51283, 39.…\n$ psk12t0          <dbl> 23.66914, 29.21420, 30.79812, 13.90037, 23.…\n$ av_n1_t0         <dbl> 4, 3, 5, 5, 2, 3, 4, 8, 4, 5, 5, 4, 5, 2, 2…\n$ av_n2_t0         <dbl> 5, 5, 1, 8, 4, 2, 4, 9, 4, 7, 4, 5, 6, 3, 3…\n$ av_n3_t0         <dbl> 8, 5, 9, 9, 8, 8, 7, 9, 4, 5, 3, 8, 6, 6, 4…\n$ av_n4_t0         <dbl> 8, 6, 7, 9, 9, 3, 6, 9, 3, 8, 2, 3, 6, 5, 4…\n$ av_n5_t0         <dbl> 3, 5, 3, 3, 3, 6, 5, 3, 5, 4, 4, 4, 4, 6, 4…\n$ av_n6_t0         <dbl> 8, 8, 8, 8, 9, 6, 4, 7, 3, 4, 3, 2, 9, 7, 7…\n$ av_n7_t0         <dbl> 7, 4, 2, 6, 1, 1, 5, 3, 5, 6, 1, 8, 1, 3, 1…\n$ av_n8_t0         <dbl> 3, 3, 3, 1, 1, 3, 6, 4, 7, 4, 6, 5, 3, 1, 3…\n$ av_n9_t0         <dbl> 5, 2, 3, 6, 1, 3, 3, 7, 5, 9, 3, 5, 1, 8, 3…\n$ av_n10_t0        <dbl> 1, 1, 1, 2, 1, 1, 2, 4, 4, 1, 4, 5, 1, 4, 2…\n$ av_n11_t0        <dbl> 2, 2, 1, 3, 2, 2, 3, 4, 6, 6, 6, 4, 3, 2, 3…\n$ sibar_risk       <dbl> 10.0, 8.0, 13.0, 5.0, 11.0, 5.0, 6.0, 2.5, …\n$ sibar_risk_di    <dbl+lbl> 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n# Pull out fake t1 variables that are actually at baseline\n\noutcome <- data %>% \n  dplyr::select(spe_sum_t3) # %>% \n  # glimpse()\n\ndata_updt <- data_init %>% \n  bind_cols(outcome) %>% \n   mutate(across(\n    c(sibar1at0:schulab, imputation,cond,sibar_risk_di),\n    as.factor\n  )) %>% \n  filter(imputation == \"0\" & !is.na(spe_sum_t3)) %>%\n  mutate(id = row_number()) %>% \n  relocate(id, everything()) %>% \n  glimpse()\n\n\nRows: 445\nColumns: 29\n$ id               <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, …\n$ imputation       <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cond             <fct> 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2…\n$ sibar1bt0        <dbl> 47, 47, 40, 31, 53, 54, 48, 49, 40, 31, 35,…\n$ sibar1at0        <fct> 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2…\n$ indikation       <fct> 1, 1, 1, 1, 1, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1…\n$ schulab          <fct> 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 1, 2, 1, 2…\n$ behandlungsdauer <dbl> NA, NA, NA, NA, NA, NA, NA, 8, 7, 7, 7, 7, …\n$ spe_sum_t0       <dbl> 3, 2, 3, 0, 1, 0, 2, 1, 3, 0, 1, 2, 1, 1, 1…\n$ phq_dept0        <dbl> 21, 25, 23, 21, 11, 11, 10, 3, 6, 15, 13, 1…\n$ phq_som_t0       <dbl> 18.000000, 26.785714, 19.000000, 13.000000,…\n$ phq_stress_t0    <dbl> 16.000000, 17.000000, 15.000000, 15.000000,…\n$ gad7t0           <dbl> 15, 21, 15, 13, 10, 12, 5, 6, 14, 11, 11, 2…\n$ ksk12t0          <dbl> 31.59306, 50.51283, 39.60604, 52.61199, 36.…\n$ psk12t0          <dbl> 29.21420, 13.90037, 23.51996, 15.58050, 27.…\n$ av_n1_t0         <dbl> 3, 5, 2, 3, 4, 5, 5, 4, 2, 2, 3, 5, 4, 5, 4…\n$ av_n2_t0         <dbl> 5, 8, 4, 2, 4, 7, 4, 5, 3, 3, 5, 4, 2, 5, 2…\n$ av_n3_t0         <dbl> 5, 9, 8, 8, 7, 5, 3, 8, 6, 4, 8, 5, 8, 6, 3…\n$ av_n4_t0         <dbl> 6, 9, 9, 3, 6, 8, 2, 3, 5, 4, 6, 5, 4, 8, 3…\n$ av_n5_t0         <dbl> 5, 3, 3, 6, 5, 4, 4, 4, 6, 4, 3, 6, 4, 4, 7…\n$ av_n6_t0         <dbl> 8, 8, 9, 6, 4, 4, 3, 2, 7, 7, 9, 6, 5, 9, 4…\n$ av_n7_t0         <dbl> 4, 6, 1, 1, 5, 6, 1, 8, 3, 1, 1, 4, 1, 4, 4…\n$ av_n8_t0         <dbl> 3, 1, 1, 3, 6, 4, 6, 5, 1, 3, 1, 6, 1, 4, 4…\n$ av_n9_t0         <dbl> 2, 6, 1, 3, 3, 9, 3, 5, 8, 3, 1, 5, 1, 3, 3…\n$ av_n10_t0        <dbl> 1, 2, 1, 1, 2, 1, 4, 5, 4, 2, 1, 4, 1, 2, 4…\n$ av_n11_t0        <dbl> 2, 3, 2, 2, 3, 6, 6, 4, 2, 3, 2, 5, 1, 4, 4…\n$ sibar_risk       <dbl> 8.0, 5.0, 11.0, 5.0, 6.0, 1.0, 3.0, 2.5, 9.…\n$ sibar_risk_di    <fct> 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0…\n$ spe_sum_t3       <dbl> 3, 0, 3, 0, 0, 1, 0, 2, 2, 0, 0, 2, 2, 2, 2…\n\nlibrary(skimr)\n# skim(data_updt)\n\n\n\nFor reasons I’ll go into more in a subsequent post, I want to create predictions separately for folks in the intervention group and folks in the control group. The short version of why: I want to be able to predict how well I would expect people to respond not just to the intervention they actually received, but how well the model thinks they would have done if they had received the other intervention. Building separate models by treatment condition isn’t the only way to do this, but it seems like the cleanest/least likely to lead to data leakage that I can think of right now5  On top of that, I’ll need to test whether those hypothetical predictions actually generalize to hold out data. So, we need to hold out at least some folks from both intervention groups to ultimately test whether our predictions about the “optimal intervention” are any good. If I could turn back time, I would just use the “initial_split” argument from the rsample package within tidymodels to create the test sets for both the intervention and control groups. That’s primarily the case because I’m not 100% sure this is reproducible and I’m not stratifying by the outcome6\n\n\n# Breaking out the predictions into the separate groups\n\ndata_intv_init <- data_updt %>% \n  filter(cond == \"2\") %>% \n  as.data.frame() # %>% \n  #glimpse()\n\ndata_ctrl_init <- data_updt %>% \n  filter(cond == \"1\") %>% \n  as.data.frame() # %>% \n  #glimpse()\n\n# Pulling out folks for test set later\n\ntest_intv <- data_intv_init %>% \n  slice_sample(n = 40)\n\ndata_intv <- data_intv_init %>% \n  anti_join(test_intv, by = \"id\") #%>% \n  #glimpse()\n\ntest_ctrl <- data_ctrl_init %>% \n  slice_sample(n = 40)\n\ndata_ctrl <- data_ctrl_init %>% \n  anti_join(test_ctrl, by = \"id\") #%>% \n  #glimpse()\n\n\n\nThen I used the recipes package to preprocess the data! I wanted to remove imputation (which didn’t vary/wasn’t relevant), designate id as an id variable rather than a predictor, one hot encode the categorical variables7, excluded near-zero variance predictors, normalized all predictors, and imputed any missing data in predictors using k-nearest neighbors.  I then set up a tidymodels workflow using an out of the box xgboost model8 and made my predictions for the intervention group first.\n\n\nintv_rec <-\n  recipe(spe_sum_t3 ~ ., data = data_intv) %>%\n  step_rm(imputation) %>% \n  update_role(id, new_role = \"id\") %>% \n  step_other(all_nominal_predictors()) %>% \n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% \n  step_nzv(all_predictors()) %>% \n  step_normalize(all_predictors()) %>% \n  step_impute_knn(all_predictors())\n\n# Testing the recipe to make sure it produces data\n\nintv_rec %>% \n  prep(verbose = TRUE) %>% \n  bake(new_data = NULL) # %>% \n\n\noper 1 step rm [training] \noper 2 step other [training] \noper 3 step dummy [training] \noper 4 step nzv [training] \noper 5 step normalize [training] \noper 6 step impute knn [training] \nThe retained training set is ~ 0.06 Mb  in memory.\n# A tibble: 173 x 33\n      id sibar1bt0 behandlungsdauer spe_sum_t0 phq_dept0 phq_som_t0\n   <int>     <dbl>            <dbl>      <dbl>     <dbl>      <dbl>\n 1     2   -0.326             0.805     0.930     2.41        2.79 \n 2     3   -1.38              1.14      1.94      2.08        1.33 \n 3     6    0.724            -0.866    -1.08      0.0881     -0.725\n 4     7   -0.176            -1.03      0.930    -0.0777      0.210\n 5     8   -0.0260            3.15     -0.0756   -1.24       -0.538\n 6     9   -1.38              2.31      1.94     -0.741      -0.164\n 7    10   -2.72              2.31     -1.08      0.752      -0.538\n 8    12   -2.87              2.31      0.930     0.420       0.383\n 9    13   -0.0260            2.31     -0.0756    2.74        1.14 \n10    14   -1.83              2.31     -0.0756    1.25        0.383\n# … with 163 more rows, and 27 more variables: phq_stress_t0 <dbl>,\n#   gad7t0 <dbl>, ksk12t0 <dbl>, psk12t0 <dbl>, av_n1_t0 <dbl>,\n#   av_n2_t0 <dbl>, av_n3_t0 <dbl>, av_n4_t0 <dbl>, av_n5_t0 <dbl>,\n#   av_n6_t0 <dbl>, av_n7_t0 <dbl>, av_n8_t0 <dbl>, av_n9_t0 <dbl>,\n#   av_n10_t0 <dbl>, av_n11_t0 <dbl>, sibar_risk <dbl>,\n#   spe_sum_t3 <dbl>, sibar1at0_X1 <dbl>, sibar1at0_X2 <dbl>,\n#   indikation_X1 <dbl>, indikation_X2 <dbl>, indikation_X3 <dbl>,\n#   schulab_X1 <dbl>, schulab_X2 <dbl>, schulab_X3 <dbl>,\n#   sibar_risk_di_X0 <dbl>, sibar_risk_di_X1 <dbl>\n\n  #skim()\n\n# Create a model\n\nxg_mod <- boost_tree() %>% \n  set_engine(\"xgboost\") %>% \n  set_mode(\"regression\")\n\n# Creating a workflow\n\nxg_wf <-\n  workflow() %>% \n  add_recipe(intv_rec) %>% \n  add_model(xg_mod)\n\n\n\n\n\nintv_folds <- vfold_cv(data_intv, v = 5, repeats = 5, strata = spe_sum_t3)\n\nkeep_pred <- control_resamples(save_pred = TRUE)\n\nlibrary(tictoc)\ntic()\nset.seed(33)\nxg_rs <-\n  xg_wf %>% \n  fit_resamples(intv_folds, control = keep_pred)\ntoc()\n\n\n10.855 sec elapsed\n\nOverall we predictions that aren’t terribly far off from the predictions of changes in anxiety and depression from Nick’s original paper (which used an ensemble model stacked on a bunch of base learners). We can’t make an apples to oranges comparison since he and his co-author were predicting different outcomes, but at least I’m not wildly off from where they were.\n\n\nxg_rs %>% \n  collect_metrics(summarize = TRUE)\n\n\n# A tibble: 2 x 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.967    25  0.0177 Preprocessor1_Model1\n2 rsq     standard   0.193    25  0.0202 Preprocessor1_Model1\n\nr2_intv <- xg_rs %>% \n  collect_metrics(summarize = TRUE) %>% \n  filter(str_detect(.metric, \"rsq\") == TRUE) %>% \n  dplyr::select(mean) %>% \n  deframe() \n\nsqrt(r2_intv)\n\n\n[1] 0.4387755\n\nintv_preds <- xg_rs %>% \n  collect_predictions(summarize = TRUE)\n\n# In order to get predictions in the placebo group I have to fit a workflow first\n\n# fit_workflow <- fit(xg_wf, data_ctrl)\n\n\n\nI then do the same preprocessing and model fitting for the control group!\n\n\nctrl_rec <-\n  recipe(spe_sum_t3 ~ ., data = data_ctrl) %>%\n  step_rm(imputation) %>% \n  update_role(id, new_role = \"id\") %>% \n  step_other(all_nominal_predictors()) %>% \n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% \n  step_nzv(all_predictors()) %>% \n  step_normalize(all_predictors()) %>% \n  step_impute_knn(all_predictors())\n\n# Testing the recipe to make sure it produces data\n\nctrl_rec %>% \n  prep(verbose = TRUE) %>% \n  bake(new_data = NULL) #%>% \n\n\noper 1 step rm [training] \noper 2 step other [training] \noper 3 step dummy [training] \noper 4 step nzv [training] \noper 5 step normalize [training] \noper 6 step impute knn [training] \nThe retained training set is ~ 0.06 Mb  in memory.\n# A tibble: 192 x 33\n      id sibar1bt0 behandlungsdauer spe_sum_t0 phq_dept0 phq_som_t0\n   <int>     <dbl>            <dbl>      <dbl>     <dbl>      <dbl>\n 1     4   -2.60               1.16     -1.26      1.88      0.313 \n 2    11   -2.03               2.64     -0.199     0.481    -0.672 \n 3    16   -0.191              1.72      0.863     0.830     1.10  \n 4    19   -0.0487             1.72     -0.199     0.133    -0.0813\n 5    20    0.802              1.72     -0.199     0.656     1.10  \n 6    21    0.0931             1.72      0.863     0.656    -0.869 \n 7    22   -1.61               1.72     -0.199     1.70      0.904 \n 8    23   -1.61               1.72     -0.199    -0.565    -0.672 \n 9    24    0.518              1.72      1.93      0.656     0.313 \n10    25   -1.75               1.72      0.863     0.656    -0.278 \n# … with 182 more rows, and 27 more variables: phq_stress_t0 <dbl>,\n#   gad7t0 <dbl>, ksk12t0 <dbl>, psk12t0 <dbl>, av_n1_t0 <dbl>,\n#   av_n2_t0 <dbl>, av_n3_t0 <dbl>, av_n4_t0 <dbl>, av_n5_t0 <dbl>,\n#   av_n6_t0 <dbl>, av_n7_t0 <dbl>, av_n8_t0 <dbl>, av_n9_t0 <dbl>,\n#   av_n10_t0 <dbl>, av_n11_t0 <dbl>, sibar_risk <dbl>,\n#   spe_sum_t3 <dbl>, sibar1at0_X1 <dbl>, sibar1at0_X2 <dbl>,\n#   indikation_X1 <dbl>, indikation_X2 <dbl>, indikation_X3 <dbl>,\n#   schulab_X1 <dbl>, schulab_X2 <dbl>, schulab_X3 <dbl>,\n#   sibar_risk_di_X0 <dbl>, sibar_risk_di_X1 <dbl>\n\n  # skim()\n\n# Create a model\n\nxg_mod <- boost_tree() %>% \n  set_engine(\"xgboost\") %>% \n  set_mode(\"regression\")\n\n# Creating a workflow\n\nxg_wf_ctrl <-\n  workflow() %>% \n  add_recipe(ctrl_rec) %>% \n  add_model(xg_mod)\n\n\n\n\n\nctrl_folds <- vfold_cv(data_ctrl, v = 5, repeats = 5, strata = spe_sum_t3)\n\nkeep_pred <- control_resamples(save_pred = TRUE)\n\nlibrary(tictoc)\ntic()\nset.seed(33)\nxg_rs_ctrl <-\n  xg_wf_ctrl %>% \n  fit_resamples(ctrl_folds, control = keep_pred)\ntoc()\n\n\n10.639 sec elapsed\n\nOverall the predictions for the control group aren’t quite as good, but we’re still doing ok relative to the original predictions on other kinds of outcomes from Nick’s paper.\n\n\nxg_rs_ctrl %>% \n  collect_metrics(summarize = TRUE)\n\n\n# A tibble: 2 x 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   1.08     25  0.0203 Preprocessor1_Model1\n2 rsq     standard   0.164    25  0.0175 Preprocessor1_Model1\n\nr2_ctrl <- xg_rs_ctrl %>% \n  collect_metrics(summarize = TRUE) %>% \n  filter(str_detect(.metric, \"rsq\") == TRUE) %>% \n  dplyr::select(mean) %>% \n  deframe() \n\nsqrt(r2_ctrl)\n\n\n[1] 0.4047978\n\nctrl_preds <- xg_rs_ctrl %>% \n  collect_predictions(summarize = TRUE)\n\n\n\nWhat We’ll Get To in Part 2\nIn a non-Dash post, I’ll write a part 2 of this dataset/analysis where I finish out the modeling process.  So now, we need to use these models to predict how well we think folks would do if they had ended up in the opposite intervention (since we already have predictions of how they would do in the intervention they actually got). Once we have those estimates, we’ll be able to see if folks in a hold out sample who we predict would benefit more from one intervention compared to another actually do!\n\nYikes↩︎\ndouble yikes↩︎\nOr at least make a good start, I don’t think there’s any way I’ll finish in one 2 hour chunk↩︎\nOr at least respond just as well↩︎\nI’m looking forward to continuing to learn more about this area and might change my mind!↩︎\nWith the 2 hour time limit I didn’t have time to re-write the code!↩︎\nlike dummy coding, but everything gets its own set of codes plus it doesn’t sound vaguely ableist↩︎\nno time for tuning in Data Distillery Dash!↩︎\n",
    "preview": {},
    "last_modified": "2021-05-17T12:03:05-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-06-data-distillery-dash-episode-01/",
    "title": "Data Distillery Dash Episode 01",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Michael Mullarkey, PhD (Click to return to my website)",
        "url": "https://mcmullarkey.github.io"
      }
    ],
    "date": "2021-05-06",
    "categories": [],
    "contents": "\nWhat Is Data Distillery Dash?\nI take a research topic at random1, wrangle the data, I create a predictive model or two, and write up a blog post: all within 2 hours live on Twitch! All the words and code in this post were written during an ~2 hour livestream. If you want to come join the party, I plan on doing this on Thursdays from 3:30-5:30 EST here.\nWe Let R Choose the Research Topic!\nOriginally I was thinking of rolling a die, but since R can choose, we let it!\n\n\ntopics <- c(\"EFG vs. TS%\",\"Off Rb% vs. Def Rb%\",\"Turnover Chaos\")\n\nset.seed(23)\ntopic_is <- sample(topics, 1)\ntopic_is\n\n\n[1] \"EFG vs. TS%\"\n\nWhat’s the Research Topic?\nAlright, so based off of this we decided to predict the difference between true shooting percentage2 and effective field goal percentage3 for NBA players from 1999-2019.4 To make things more interesting/not overfit completely, we dropped any variables related to free throw shooting. We also dropped true shooting percentage and effective field goal percentage themselves, since those together would be redundant with our outcome of interest.  First, let’s load in all the packages we’ll use during the livestream5\n\n\nlibrary(nbastatR)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(doMC)\nlibrary(tictoc)\nlibrary(future)\nlibrary(janitor)\nlibrary(skimr)\n\n\n\nAnd now let’s use nbastatR to load in the advanced statistics and the total stats for all NBA players over ~20 years.\n\n\ntic()\nplan(multisession)\nall_bref <- bref_players_stats(seasons = 2000:2019, tables = c(\"advanced\",\"totals\"), widen = TRUE, assign_to_environment = TRUE)\n\n\nparsed http://www.basketball-reference.com/leagues/NBA_2000_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2001_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2002_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2003_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2004_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2005_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2006_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2007_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2008_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2009_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2010_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2011_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2012_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2013_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2014_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2015_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2016_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2017_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2018_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2019_advanced.html\nparsed http://www.basketball-reference.com/leagues/NBA_2000_totals.html\nparsed http://www.basketball-reference.com/leagues/NBA_2001_totals.html\nparsed http://www.basketball-reference.com/leagues/NBA_2002_totals.html\nparsed http://www.basketball-reference.com/leagues/NBA_2003_totals.html\nparsed http://www.basketball-reference.com/leagues/NBA_2004_totals.html\nparsed http://www.basketball-reference.com/leagues/NBA_2005_totals.html\nparsed http://www.basketball-reference.com/leagues/NBA_2006_totals.html\nparsed http://www.basketball-reference.com/leagues/NBA_2007_totals.html\nparsed http://www.basketball-reference.com/leagues/NBA_2008_totals.html\nparsed http://www.basketball-reference.com/leagues/NBA_2009_totals.html\nparsed http://www.basketball-reference.com/leagues/NBA_2010_totals.html\nparsed http://www.basketball-reference.com/leagues/NBA_2011_totals.html\nparsed http://www.basketball-reference.com/leagues/NBA_2012_totals.html\nparsed http://www.basketball-reference.com/leagues/NBA_2013_totals.html\nparsed http://www.basketball-reference.com/leagues/NBA_2014_totals.html\nparsed http://www.basketball-reference.com/leagues/NBA_2015_totals.html\nparsed http://www.basketball-reference.com/leagues/NBA_2016_totals.html\nparsed http://www.basketball-reference.com/leagues/NBA_2017_totals.html\nparsed http://www.basketball-reference.com/leagues/NBA_2018_totals.html\nparsed http://www.basketball-reference.com/leagues/NBA_2019_totals.html\nAdvanced\nTotals\n\ntoc()\n\n\n52.136 sec elapsed\n\nHere Was My Initial Frantic Plan\nDo data wrangling (Create target variable)  Get minimal model pipeline (Includes EDA)  Go back and iterate on modeling  TWIST6  Making more plots if we have time, and building out the rest of the blog post\n\n\n\n\n\n\nWhy Bother with This Question?\nEven though true shooting percentage and effective field goal percentage are similar, looking at their differences might be informative. The main difference between the two, is true shooting percentage takes into account free throw percentage, while effective field goal percentage does not.  Still, the metrics are correlated r = 0.92 in our data!7 So the cases where they differ by a lot might be rare, but could be really interesting.\n\n\nlibrary(corrr)\n\nall_bref_wrang %>%\n  dplyr::select(pct_true_shooting, pct_efg) %>% \n  correlate()\n\n\n# A tibble: 2 x 3\n  term              pct_true_shooting pct_efg\n  <chr>                         <dbl>   <dbl>\n1 pct_true_shooting            NA       0.921\n2 pct_efg                       0.921  NA    \n\nWho Has the Biggest Gaps Between True Shooting Percentage and Effective Field Goal Percentage?\nWe can also look at who has the biggest gaps between true shooting percentage and effective field goal percentage.  Folks who have a true shooting percentage much higher than their effective field goal percentage8 are much better free throw shooters than they are shooting from the field. Folks who have a much higher effective field goal percentage than true shooting percentage9 are much worse free throw shooters compared to how they shoot from the field.  It’s interesting to me that while the data goes through the 2018-2019 season, none of the top 10 discrepancies happen after the 2011-2012 season. Could just be chance for sure, but might be worth investigating further!\n\n# A tibble: 10 x 3\n   name_player     ts_minus_efg slug_season\n   <chr>                  <dbl> <chr>      \n 1 Danny Fortson          0.16  2004-05    \n 2 Kevin Ollie            0.133 2003-04    \n 3 Tony Farmer            0.122 1999-00    \n 4 Corey Maggette         0.121 2006-07    \n 5 Kevin Ollie            0.12  2001-02    \n 6 Kevin Ollie            0.118 2008-09    \n 7 Corey Maggette         0.117 2004-05    \n 8 Corey Maggette         0.112 2011-12    \n 9 Charles Oakley         0.111 2002-03    \n10 Antonio Daniels        0.111 2005-06    \n\nAlso, yikes is Deandre Jordan terrible at shooting free throws relative to how he shoots from the field! I know we already knew that, but still. He takes up the top 4 spots on the “true shooting percentage lower than effective field goal percentage” list, and 6 of the top 10 overall.\n\n# A tibble: 10 x 3\n   name_player     ts_minus_efg slug_season\n   <chr>                  <dbl> <chr>      \n 1 DeAndre Jordan       -0.0750 2015-16    \n 2 DeAndre Jordan       -0.0730 2014-15    \n 3 DeAndre Jordan       -0.0480 2008-09    \n 4 DeAndre Jordan       -0.0460 2013-14    \n 5 Joey Dorsey          -0.0450 2014-15    \n 6 Andris Biedrins      -0.0430 2005-06    \n 7 Brendan Haywood      -0.0420 2010-11    \n 8 DeAndre Jordan       -0.0410 2016-17    \n 9 DeAndre Jordan       -0.0380 2010-11    \n10 DeAndre Jordan       -0.0370 2012-13    \n\nIs The Outcome Distributed in A Super Wonky Way?\nOverall though, the difference between true shooting percentage and effective field goal percentage seems pretty normally distributed.\n\n\nall_bref_wrang %>% \n  na.omit() %>% \n  ggplot(aes(x = ts_minus_efg)) + \n  labs(x = \"TS - EFG\") +\n  geom_histogram(alpha = 0.7)\n\n\n\n\nSome EDA + Modeling Time\nI then created a minimal modeling pipeline with the tidymodels framework! Shout out to Max Kuhn and Julia Silge + others at RStudio forever.  I also split the data into training and testing, though this post will only look at the model performance (via resampling) in the training data.\n\n\nbref_pred <- all_bref_wrang %>% \n  dplyr::select(name_player, ts_minus_efg, c(minutes:pts_totals)) %>% \n  dplyr::select(-ftm_totals, -fta_totals,-pct_ft_rate,-pct_ft, -pct_true_shooting, -pct_efg)\n\n# glimpse(bref_pred)\n\n# It's SPLIT time\n\nset.seed(33)\nbref_split <- initial_split(bref_pred, prop = 4/5, strata = ts_minus_efg) \n\nbref_train <- training(bref_split)\nbref_test <- testing(bref_split)\n\n\n\nI originally wrote code to look at all the univariate associations between the predictors (Basketball Reference Data for all the players over the past 20 years) and the outcome in the training data. This isn’t to do any screening, but helps me get a sense for how well a simple linear model might do. For example, if there were a bunch of strong positive/negative linear associations with the outcome, the linear regression might beat more sophisticated, compuationally intensive models. That happened a little bit here (spoilers!) and here’s an example of one plot.10\n\n\n# preds <- bref_train %>% \n#   dplyr::select(where(is.numeric)) %>% \n#   names()\n# \n# map(preds, ~{\n#   \n#   bref_train %>% \n#     ggplot(aes(x = .data[[.x]], y = ts_minus_efg)) +\n#     geom_point(alpha = 0.2, position = \"jitter\") +\n#     geom_smooth(method = lm, formula = y ~ x, se = FALSE, col = \"red\")\n#   \n# })\n\nbref_train %>% \n    ggplot(aes(x = minutes, y = ts_minus_efg)) +\n    geom_point(alpha = 0.2, position = \"jitter\") +\n    geom_smooth(method = lm, formula = y ~ x, se = FALSE, col = \"red\") +\n    labs(y = \"True Shooting % - EFG%\")\n\n\n\n\nHere come the recipes! Nope, the food I make isn’t tasty, but at least this recipe can take care of any missing data or near-zero variance predictors. I also decided to train a no-frills linear regression along with an out of the box boosted tree model.11\n\n\nts_efg_rec <- recipe(ts_minus_efg ~ ., data = bref_train) %>% \n  update_role(name_player, new_role = \"id\") %>% \n  step_impute_knn(all_numeric(), -all_outcomes()) %>% \n  step_normalize(all_numeric(), -all_outcomes()) %>% \n  step_nzv(all_numeric(),-all_outcomes())\n\nts_efg_rec %>% \n  prep(verbose = TRUE) %>% \n  bake(new_data = bref_train)\n\n\noper 1 step impute knn [training] \noper 2 step normalize [training] \noper 3 step nzv [training] \nThe retained training set is ~ 1.75 Mb  in memory.\n# A tibble: 5,356 x 41\n   name_player    minutes ratio_per pct3p_rate pct_orb pct_drb pct_trb\n   <fct>            <dbl>     <dbl>      <dbl>   <dbl>   <dbl>   <dbl>\n 1 Tariq Abdul-W… -0.126    -0.152     -0.988  -0.0205  -0.277 -0.0132\n 2 Shareef Abdur…  2.16      1.42      -0.794   0.0646   1.40   1.17  \n 3 Ray Allen       1.95      1.52       0.263  -0.344   -0.730 -0.728 \n 4 John Amaechi    0.0214   -0.248     -1.12   -0.276   -0.259 -0.326 \n 5 Derek Anderson  0.739     0.634     -0.139  -0.285   -0.939 -0.795 \n 6 Kenny Anderson  1.28      0.754     -0.0594 -0.421   -1.18  -1.15  \n 7 Nick Anderson   0.591    -0.581      1.36   -0.268   -0.329 -0.371 \n 8 Chris Anstey   -0.919     0.205     -1.09    0.261    1.36   1.39  \n 9 Darrell Armst…  1.28      1.25       0.621  -0.387   -1.06  -0.974 \n10 Chucky Atkins  -0.0591    0.0384    -0.0743 -0.506   -1.32  -1.31  \n# … with 5,346 more rows, and 34 more variables: pct_ast <dbl>,\n#   pct_stl <dbl>, pct_blk <dbl>, pct_tov <dbl>, pct_usg <dbl>,\n#   ratio_ows <dbl>, ratio_dws <dbl>, ratio_ws <dbl>,\n#   ratio_ws_per48 <dbl>, ratio_obpm <dbl>, ratio_dbpm <dbl>,\n#   ratio_bpm <dbl>, ratio_vorp <dbl>, count_games_started <dbl>,\n#   pct_fg <dbl>, pct_fg3 <dbl>, pct_fg2 <dbl>, minutes_totals <dbl>,\n#   fgm_totals <dbl>, fga_totals <dbl>, fg3m_totals <dbl>,\n#   fg3a_totals <dbl>, fg2m_totals <dbl>, fg2a_totals <dbl>,\n#   orb_totals <dbl>, drb_totals <dbl>, trb_totals <dbl>,\n#   ast_totals <dbl>, stl_totals <dbl>, blk_totals <dbl>,\n#   tov_totals <dbl>, pf_totals <dbl>, pts_totals <dbl>,\n#   ts_minus_efg <dbl>\n\n# It's peanut butter model time\n\nlm_mod <- linear_reg() %>% \n  set_engine(\"lm\")\n\nxg_mod <- boost_tree() %>% \n  set_engine(\"xgboost\") %>% \n  set_mode(\"regression\")\n\n# Working hard or harding workflowing\n\nrec_list <- list(ts_efg_rec, ts_efg_rec)\n\nmod_list <- list(lm_mod, xg_mod)\n\nbase_wfs <- map2(rec_list, mod_list, ~{\n  \n  wf <-\n    workflow() %>% \n    add_recipe(.x) %>% \n    add_model(.y)\n  \n})\n\ndata_list <- list(bref_train, bref_train)\n\nbase_one <- map2(base_wfs, data_list, ~{\n  \n  tic()\n  wf_fit <- fit(.x, data = .y)\n  toc()\n  wf_fit\n  \n})\n\n\n0.523 sec elapsed\n0.782 sec elapsed\n\nEvaluting the Models Using 10-Fold Cross-Validation\nThen I evaluated both models using 10 fold cross-validation to make sure I’m not overfitting too much.12\n\n\n# Resampling time!!\n\nbase_mod_rs <- map2(base_wfs, data_list, ~{\n  \n  registerDoMC(cores = 7)\n  \n  set.seed(33)\n  folds <- vfold_cv(.y, v = 10, repeats = 10, strata = ts_minus_efg)\n  keep_pred <- control_resamples(save_pred = TRUE)\n  tic()\n  set.seed(33)\n  fit_rs <-\n    .x %>% \n    fit_resamples(folds, control = keep_pred)\n  toc()\n  fit_rs\n  \n})\n\n\n12.745 sec elapsed\n15.668 sec elapsed\n\nHow Did the Models Do?\nBoth models did about the same as one another according to RMSE! That would make me prefere the simpler linear model, though I’m 100% sure these models could be improved. If I had more time, I would have done some tunining of the xgboost model, done some more visualizations to assist with feature engineering, and maybe even created some spicy memes.13\n\n\nbase_metrics <- map(base_mod_rs, ~{\n  \n  .x %>% \n    collect_metrics(summarize = TRUE)\n  \n}) %>% \n  print()\n\n\n[[1]]\n# A tibble: 2 x 6\n  .metric .estimator    mean     n   std_err .config             \n  <chr>   <chr>        <dbl> <int>     <dbl> <chr>               \n1 rmse    standard   0.00925   100 0.0000399 Preprocessor1_Model1\n2 rsq     standard   0.778     100 0.00182   Preprocessor1_Model1\n\n[[2]]\n# A tibble: 2 x 6\n  .metric .estimator   mean     n   std_err .config             \n  <chr>   <chr>       <dbl> <int>     <dbl> <chr>               \n1 rmse    standard   0.0138   100 0.0000426 Preprocessor1_Model1\n2 rsq     standard   0.528    100 0.00297   Preprocessor1_Model1\n\n\n\nbase_preds <- map(base_mod_rs, ~{\n  \n  .x %>% \n    collect_predictions(summarize = TRUE)\n  \n}) %>% \n  print()\n\n\n[[1]]\n# A tibble: 5,356 x 4\n    .row ts_minus_efg .config               .pred\n   <int>        <dbl> <chr>                 <dbl>\n 1     1      0.051   Preprocessor1_Model1 0.0487\n 2     2      0.0700  Preprocessor1_Model1 0.0745\n 3     3      0.0540  Preprocessor1_Model1 0.0564\n 4     4      0.067   Preprocessor1_Model1 0.0682\n 5     5      0.0720  Preprocessor1_Model1 0.0695\n 6     6      0.0410  Preprocessor1_Model1 0.0470\n 7     7      0.00300 Preprocessor1_Model1 0.0113\n 8     8      0.068   Preprocessor1_Model1 0.0556\n 9     9      0.0480  Preprocessor1_Model1 0.0353\n10    10      0.0270  Preprocessor1_Model1 0.0346\n# … with 5,346 more rows\n\n[[2]]\n# A tibble: 5,356 x 4\n    .row ts_minus_efg .config               .pred\n   <int>        <dbl> <chr>                 <dbl>\n 1     1      0.051   Preprocessor1_Model1 0.0540\n 2     2      0.0700  Preprocessor1_Model1 0.0668\n 3     3      0.0540  Preprocessor1_Model1 0.0588\n 4     4      0.067   Preprocessor1_Model1 0.0580\n 5     5      0.0720  Preprocessor1_Model1 0.0587\n 6     6      0.0410  Preprocessor1_Model1 0.0500\n 7     7      0.00300 Preprocessor1_Model1 0.0288\n 8     8      0.068   Preprocessor1_Model1 0.0564\n 9     9      0.0480  Preprocessor1_Model1 0.0566\n10    10      0.0270  Preprocessor1_Model1 0.0479\n# … with 5,346 more rows\n\nWe see the predictions primarily break down at the extreme ends of each distribution, though the boosted tree model starts breaking down even before then.\n\n\nmap(base_preds, ~{\n  \n  .x %>% \n    ggplot(aes(x = .pred, y = ts_minus_efg)) +\n    geom_point(alpha = 0.2, position = \"jitter\")\n  \n}) %>% \n  print()\n\n\n[[1]]\n\n\n[[2]]\n\n\nFinal Thoughts\nThis was really fun! It came down to the wire for me to finish this post, but I had an absolute blast. I’m looking forward to doing this again. If you enjoyed this, come follow me on Twitch\n\nYikes↩︎\nwhich takes into account free throw percentage along with giving people more credit for making 3s↩︎\nwhich gives people more credit for making 3s but doesn’t give people credit for free throws↩︎\nshout out to the nbastatR package for making getting that data easy↩︎\nor almost all of them!↩︎\nJust kidding, tune in next time to see if I remember to do it!↩︎\namong players who played at least 500 minutes in a given season, total sample size across 20 years of 6,691 players↩︎\npositive values by this metric↩︎\nnegative values by this metric↩︎\nThe commented out code creates all the plots↩︎\nI know workflowsets exist, but I haven’t worked with them yet! Maybe next time…↩︎\nYes, I know there’s some controversey over how much k-fold cross-validation helps with that, but I’m doing what I know how to do!↩︎\nshout out to Tony on Sliced, a competitive data science stream <twitch.tv/nickwan_datasci>↩︎\n",
    "preview": "posts/2021-05-06-data-distillery-dash-episode-01/data-distillery-dash-episode-01_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2021-05-17T12:06:52-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-04-party-drugs-lies-and-statistics/",
    "title": "Party Drugs, Lies, and Statistics",
    "description": "How the party drug hype cycle is both already over and neverending",
    "author": [
      {
        "name": "Michael Mullarkey, PhD (Click to return to my website)",
        "url": "https://mcmullarkey.github.io"
      }
    ],
    "date": "2021-05-04",
    "categories": [],
    "contents": "\nI: Pretending\nLet’s pretend the hype cycle of “party drugs are miracle mental health drugs!” is already over. This time wasn’t different. The rates of mental health problems are either staying unacceptably high or climbing even higher. The well-intentioned1 scientists, once hopeful patients, opportunistic grifters, and bored VCs have all moved on to the next big thing.  How could we have gotten here?  After all, there was just a New York Times writeup of a gold-standard, placebo-controlled trial showing MDMA improved PTSD! The headline even says “A Psychedelic Drug Passes Big Test for PTSD treatment.” There have been a lot of other promising headlines too, like “Could the Embrace of Psychedelics Lead to a Mental-Health Revolution?”, “Mind Menders: The Future of Psychedelics for Mental Illness”, and “How ecstasy and psilocybin are shaking up psychiatry.” That last headline is from Nature.  Humans are bad at predicting the future. But if the party drug hype cycle ends, a lot of its downfall could be predicted by one of the first skills we develop.\nII: Counting\nThere’s a great statistics textbook I’m slogging my way through right now. A lot of its insights have come from translating scientific jargon into actionable English. At many points throughout, the author reminds us statistics is glorified counting.2 Let’s do some counting with the latest, greatest party drug trial. From the NYT article, “67 percent of participants in the MDMA group no longer qualified for a diagnosis of PTSD, compared with 32 percent in the placebo group.” That sounds super impressive! But percentages aren’t quite counting yet. There were 90 people in the trial, so let’s assume an equal number of people in both groups and that literally no one left the study.3 That means ~30 out of 45 people who took MDMA got better, while ~15 out of 45 people who took an inactive placebo got better. I’m over the moon for the ~45 folks who got better during this study, including ~30 of them who did so in the group who took MDMA. And if you’re also asking “Wait, only a high school classroom’s worth of people got better with MDMA? Is that enough to decide a drug works?” your instincts are right.\nIII: Powering\nPart of the alchemy that turns counting into helpful statistics is having enough stuff to count. Every single person’s life is valuable beyond measure. In a randomized trial of a treatment each person counts exactly once.4 We can do a power analysis — a formula that tells us how many people we need to trust our counting process — to figure out if the party drug trial had enough stuff to count.  When we run this formula and assume party drugs have effects similar to psychotherapy and other psychotropic medications5 we find this trial had nowhere near enough stuff to count. The study would need to be ~100 times larger for us to take the counts seriously.\n\n\n\nIf, however, we assume MDMA is twice as powerful as any other high-quality intervention we’ve ever seen, there’s more than enough stuff to count. We’d only have count up the responses of 78 folks to know reliably how well people responded to MDMA.\n\n\n\nIV: Pretending Again\nNow, we’re back to pretending the party drug hype cycle is over. It’s obvious now we shouldn’t have expected these drugs to be twice as powerful as anything we’ve tested before. Sure, the smaller studies showed much larger effects, as they always do. But in this world we conducted much larger trials next, and the apparent benefits shrank or disappeared. Or we never conducted larger trials, but it’s been years to decades with no obvious improvement in overall mental health.  We’re looking back with disgust at the enrollment of patients into studies without telling them, nodding knowingly about the many documented, non-counting related flaws in the “above board” studies, and already hoping the next hyped treatment will succeed where party drugs have failed. The New York Times has even written an excellent investigative piece tying together everything that went wrong. The people they interview who are still suffering break our hearts.\nV: Hoping\nI hope the world where the party drug hype cycle ends is only a fantasy. I hope against hope, despite no good evidence, that party drugs are twice as effective (or more) as anything we already have. I’m human, I hope for a lot of things that probably aren’t true.\n\nOr maybe not, who knows↩︎\nThe author is making a specific, technical point in the context of Bayesian statistics, and I think the general principle is still useful↩︎\nThat seems unlikely, but let’s give the study the benefit of the doubt↩︎\nUnless you have lots of repeated measures from individual people, but that’s just a different, more complicated kind of counting↩︎\nUsing d = 0.20 as an approximation from low risk of bias, higher sample size trials testing these treatments, see https://www.ncbi.nlm.nih.gov/books/NBK78732/, assuming the placebo response rate was equivalent to what was observed in the trial, and only giving ourselves a 5% chance of a false negative↩︎\n",
    "preview": "posts/2021-05-04-party-drugs-lies-and-statistics/party-drugs-lies-and-statistics_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-05-17T11:13:03-04:00",
    "input_file": {}
  }
]
